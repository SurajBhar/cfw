# ViT-Large/16 Model Configuration (timm)

name: vit_l_16
model_type: vit

# Model architecture
backbone: vit_l_16
embedding_dim: 1024
patch_size: 16

# Classification head
head_type: linear  # Options: linear, mlp
hidden_dims: []  # For MLP head: [512, 256]
dropout: 0.0

# Model initialization
pretrained: true
freeze_backbone: false  # Set to true for linear probing

# Output
num_classes: ${dataset.num_classes}
